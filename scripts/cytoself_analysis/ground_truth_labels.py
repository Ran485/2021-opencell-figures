import datetime
import os
import json
import requests
import pandas as pd


THIS_DIR = os.path.abspath(os.path.dirname(__file__))
DATA_DIR = os.path.join(THIS_DIR, os.pardir, os.pardir, 'data')


def cache_target_metadata(data_dirpath):
    '''
    cache the payload from the /lines endpoint
    This payload is often referred to as the 'target metadata' or simply 'metadata'
    '''
    timestamp = datetime.datetime.now().strftime('%Y-%m-%d')

    all_results_filepath = os.path.join(data_dirpath, '%s-all-opencell-lines.json' % timestamp)
    public_results_filepath = os.path.join(data_dirpath, '%s-public-opencell-lines.json' % timestamp)

    # from the private API (i.e., the db on `cap`)
    all_results = requests.get('http://opencell.cap.czbiohub.org/api/lines')
    
    # from the public API
    public_results = requests.get('https://opencell.czbiohub.org/api/lines')
    
    data = all_results.json()
    with open(all_results_filepath, 'w') as file:
        json.dump(data, file)
        
    data = public_results.json()
    with open(public_results_filepath, 'w') as file:
        json.dump(data, file)

    
def merge_all(df, data_dirpath):
    '''
    Convenience method to merge all of the ground-truth labels at once
    '''
    # note that we need to use the full set of opencell lines, not just the public set,
    # so that the 'pub_ready' column created in merge_opencell_metadata 
    # is always True or False (and never nan)
    # (this JSON file is generated by the cache_target_metadata method above)
    cached_lines_payload_filepath = os.path.join(data_dirpath, '2021-06-09-all-opencell-lines.json')
    df = merge_opencell_metadata(df, cached_lines_payload_filepath)
    df = merge_opencell_annotations(df, cached_lines_payload_filepath, only_count_grade_3=True)
    return df


def merge_opencell_metadata(df, filepath):
    '''
    Append uniprot_id, ensg_id, oc_categories, and pub_ready columns
    to the dataframe `df` using the cached /lines endpoint
    '''
    with open(filepath, 'r') as file:
        metadata = json.load(file)

    metadata = pd.DataFrame(
        data=[
            {
                'oc_categories': row['annotation']['categories'], 
                'uniprot_id': row['uniprot_metadata']['uniprot_id'],
                **row['metadata']
            }
            for row in metadata
        ]
    )

    # manually construct the pub-ready flag
    metadata['pub_ready'] = metadata.oc_categories.apply(
        lambda s: 'publication_ready' in s if s is not None else False
    )

    # drop the columns that we're about to merge to avoid duplicating them
    df.drop(
        ['uniprot_id', 'ensg_id', 'oc_categories', 'pub_ready'], 
        axis=1, 
        errors='ignore', 
        inplace=True
    )

    df = df.merge(
        metadata[['cell_line_id', 'uniprot_id', 'ensg_id', 'oc_categories', 'pub_ready']], 
        on='cell_line_id', 
        how='left'
    )
    return df


def merge_opencell_annotations(df, filepath, only_count_grade_3=True):
    '''
    Appends columns 'ocgt_label' and 'grade_3_annotation' to the dataframe df
    'ocgt_label': the grade-3 annotation for targets with only one annotation
        (of any grade -or- of only grade-3, if only_count_grade_3 is True)
    'grade_3_annotation': the first grade-3 annotation for all targets with any grade-3 annotations
        (arbitrarily chosen for targets with more than one grade-3)

    Parameters
    filepath : path to a cached cell line metadata payload from the opencell API /lines endpoint
    only_count_grade_3 : whether to count all graded annotations or only grade-3 annotations
    '''
    with open(filepath, 'r') as file:
        metadata = json.load(file)

    # move the metadata and annotation categories into a dataframe
    d = []
    for row in metadata:
        new_row = row['metadata']
        new_row['annotation'] = row['annotation']['categories']
        d.append(new_row)
    all_ants = pd.DataFrame(data=d)

    all_ants = all_ants.explode('annotation')[['cell_line_id', 'target_name', 'annotation']]
    all_ants = all_ants.dropna(axis=0, subset=['annotation'])

    # parse the annotation name and grade
    all_ants['annotation_grade'] = all_ants.annotation.apply(lambda s: s.split('_')[-1])
    all_ants['annotation_name'] = all_ants.annotation.apply(lambda s: '_'.join(s.split('_')[:-1]))

    # drop the non-graded annotations (these are 'old' annotations and QC categories)
    all_ants = all_ants.loc[all_ants.annotation_grade.isin(['1', '2', '3'])]

    if only_count_grade_3:
        ants_to_count = all_ants.loc[all_ants.annotation_grade == '3']
    else:
        ants_to_count = all_ants
    num_ants = ants_to_count.groupby('cell_line_id').count().annotation

    # the targets with only a single annotation
    cell_line_ids_one_ant = num_ants.loc[num_ants == 1].index.values

    # the single grade-3 annotations for targets with only one grade-3 annotation
    # (but possibly other grade-1 or -2 annotations, if only_count_grade_3 is True)
    single_grade_3_annotations = all_ants.loc[
        (all_ants.annotation_grade == '3') & all_ants.cell_line_id.isin(cell_line_ids_one_ant)
    ]
    
    # drop rare annotations
    min_count = 15
    ant_counts = single_grade_3_annotations.annotation_name.value_counts()
    most_common_annotations = ant_counts.loc[ant_counts > min_count].index.values
    single_grade_3_annotations = single_grade_3_annotations.loc[
        single_grade_3_annotations.annotation_name.isin(most_common_annotations)
    ]

    # these annotation names are our ground truth labels
    # ('ocgt' stands for 'opencell ground truth')
    single_grade_3_annotations.rename(columns={'annotation_name': 'ocgt_label'}, inplace=True)

    # drop the 'ocgt_label' column from the test labels if it already exists        
    df.drop('ocgt_label', axis=1, errors='ignore', inplace=True)

    # merge the annotations
    df = pd.merge(
        df, 
        single_grade_3_annotations[['cell_line_id', 'ocgt_label']], 
        left_on='cell_line_id', 
        right_on='cell_line_id', 
        how='left'
    )

    # the most common annotations 
    min_count = 15
    ant_counts = all_ants.annotation_name.value_counts()
    most_common_annotations = ant_counts.loc[ant_counts > min_count].index.values

    # merge the first grade-3 annotation for all targets (if any), as long as it is common
    column = 'grade_3_annotation'
    grade_3_annotations = (
        all_ants.loc[
            (all_ants.annotation_grade == '3') & 
            all_ants.annotation_name.isin(most_common_annotations)
        ]
        .groupby('cell_line_id')
        .first()
        .reset_index()
        .rename(columns={'annotation_name': column})
    )

    df.drop(column, axis=1, errors='ignore', inplace=True)
    df = pd.merge(
        df, 
        grade_3_annotations[['cell_line_id', column]], 
        left_on='cell_line_id', 
        right_on='cell_line_id', 
        how='left'
    )

    # merge the full comma-delimated set of common grade2/3 annotations
    column = 'grade_23_annotation_set'
    annotation_sets = (
        all_ants.loc[
            (all_ants.annotation_grade.isin(['2', '3'])) & 
            all_ants.annotation_name.isin(most_common_annotations)
        ]
        .groupby('cell_line_id')
        .agg(lambda s: ', '.join(sorted(s)))
        .reset_index()
        .rename(columns={'annotation_name': column})
    )

    df.drop(column, axis=1, errors='ignore', inplace=True)
    df = pd.merge(
        df, 
        annotation_sets[['cell_line_id', column]], 
        left_on='cell_line_id', 
        right_on='cell_line_id', 
        how='left'
    )

    # create a readable label for unlabeled targets (which is most of them)
    for column in ['ocgt_label', 'grade_3_annotation', 'grade_23_annotation_set']:
        df[column] = df[column].fillna(value='none')

    # test labels for the 'full' dataset do not have any localization labels included
    # (which the 'old' dataset did, in the label_0, label_1, ... label_5 columns)
    # here, for backwards compatibility with various plotting methods, 
    # we create the label_0 column and copy the grade-3 annotations into it
    if 'label_0' not in df.columns:
        df['label_0'] = df.grade_3_annotation.copy()
    return df


def merge_hpa_labels(df):
    '''
    Append the HPA 'main location' labels
    Columns added: 'hpa_name', 'hpa_main_location', 'hpgt_label'
    '''
    hpa = pd.read_csv(os.path.join(DATA_DIR, 'external', 'hpa_subcellular_location.tsv'), sep='\t')
    hpa.rename(columns={
            column: column.lower().replace(' ', '_') for column in hpa.columns
        },
        inplace=True
    )
    hpa.rename(columns={
            'gene': 'ensg_id', 
            'gene_name': 'hpa_name', 
            'main_location': 'hpa_main_location'
        },
        inplace=True
    )

    # hpa['go_id'] = hpa['go_id'].str.split(';')
    # hpa['main_location'] = hpa['main_location'].str.split(';')

    df.drop(
        labels=['hpa_name', 'hpa_main_location', 'hpgt_label'], 
        axis=1, 
        inplace=True,
        errors='ignore'
    )
    
    df = df.merge(
        hpa[['ensg_id', 'hpa_name', 'hpa_main_location']],
        left_on='ensg_id', 
        right_on='ensg_id', 
        how='left'
    )

    # string-typed null for the HPA columns
    for col in ['hpa_name', 'hpa_main_location']:
        df[col] = df[col].fillna(value='none')

    # take the first label when there is more than one
    df['hpgt_label'] = df.hpa_main_location.str.split(';').apply(lambda s: s[0])
    
    return df


def load_corum(drop_largest=0):
    '''
    Appends a 'corum_labels' column
    '''
    corum = pd.read_csv(
        os.path.join(DATA_DIR, 'external', 'processed_human_corum.csv'),
        index_col=0
    )

    corum.rename(columns={'complex': 'corum_label', 'gene_names': 'gene_name'}, inplace=True)

    # corum cluster sizes
    cluster_sizes = corum.corum_label.value_counts()
    
    # gene 'sizes' (number of clusters each gene is in)
    gene_sizes = corum.gene_name.value_counts()
    
    if drop_largest:
        corum = corum.loc[~corum.corum_label.isin(cluster_sizes.head(drop_largest).index)]
        print('Dropping the %s largest CORUM clusters' % drop_largest)

    return corum


def merge_corum(df, drop_largest=5):
    '''
    '''
    corum = load_corum(drop_largest=False)
    corum_wo_largest = load_corum(drop_largest=drop_largest)

    df['corum_labels'] = None
    df['corum_labels_wo_largest'] = None

    for ind, row in df.iterrows():
        df.at[ind, 'corum_labels'] = ';'.join(
            corum.loc[corum.gene_name == row.target_name].corum_label.astype(str).tolist()
        )
        df.at[ind, 'corum_labels_wo_largest'] = ';'.join(
            corum_wo_largest.loc[corum_wo_largest.gene_name == row.target_name]
            .corum_label
            .astype(str)
            .tolist()
        )

    # targets without a corum cluster end up with an empty string
    df['corum_labels'] = df.corum_labels.replace(to_replace='', value='none')
    df['corum_labels_wo_largest'] = df.corum_labels_wo_largest.replace(to_replace='', value='none')


def load_corum_standard(drop_largest=0):
    '''
    '''
    corum = load_corum(drop_largest)
    standard = corum.groupby('corum_label')['gene_name'].apply(list).to_list()
    return standard


def merge_kegg_pathways(df, drop_largest=3):
    '''
    Merge the kegg pathways on uniprot_id
    Drop the three largest pathways by default, which are all obviously generic
    '''
    pathways = load_kegg_pathways(drop_largest)
    pathways = pathways.groupby('uniprot_id').agg(';'.join).reset_index()

    df.drop(labels=['pathway_id'], axis=1, inplace=True,errors='ignore')

    # merge the pathway_ids into adata.obs
    df = pd.merge(
        df,
        pathways[['uniprot_id', 'pathway_id']],
        left_on='uniprot_id', 
        right_on='uniprot_id', 
        how='left',
    )
    df['pathway_id'] = df.pathway_id.fillna('none')
    return df


def load_kegg_pathways(drop_largest=0):
    '''
    This uses a manually-downloaded list of Kegg pathway members
    and a map from kegg gene_id to uniprot_id
    '''
    pathway_names = pd.read_csv(
        os.path.join(DATA_DIR, 'external', 'kegg-api-list-pathway-hsa.txt'), sep='\t', header=None
    )
    pathway_names.columns = ['pathway_id', 'pathway_name']

    pathway_members = pd.read_csv(
        os.path.join(DATA_DIR, 'external', 'kegg-api-link-hsa-pathway.txt'), sep='\t', header=None
    )
    pathway_members.columns = ['pathway_id', 'kegg_gene_id']

    # the kegg gene_id - uniprot_id mapping (this is many-to-many - not sure why)
    kegg_uniprot = pd.read_csv(
        os.path.join(DATA_DIR, 'external', 'kegg-api-conv-hsa-uniprot.txt'), sep='\t', header=None
    )
    kegg_uniprot.columns = ['uniprot_id', 'kegg_gene_id']

    kegg_uniprot['uniprot_id'] = kegg_uniprot.uniprot_id.apply(lambda s: s.split(':')[1])

    pathway_members = (
        pathway_members
        .merge(pathway_names, on='pathway_id', how='inner')
        .merge(kegg_uniprot, left_on='kegg_gene_id', right_on='kegg_gene_id', how='inner')
    )

    # eliminate duplicated uniprot_id - pathway_id pairs 
    # (this is needed because the kegg_gene_id - uniprot_id mapping is many-to-many)
    pathway_members = pathway_members.groupby(['pathway_id', 'uniprot_id']).first().reset_index()

    # drop the largest pathways
    largest_pathway_ids = pathway_members.pathway_id.value_counts().head(drop_largest).index
    pathway_members = pathway_members[~pathway_members.pathway_id.isin(largest_pathway_ids)]

    return pathway_members


def merge_go_slim(df):
    '''
    '''
    go_components = ['cellular_component', 'molecular_function', 'biological_process']

    goslim = load_go_slim()
    goslim = goslim.groupby(['uniprot_id', 'go_type']).agg(';'.join)

    for go_component in go_components:
        df['go_%s' % go_component] = 'none'

    for ind, row in df.iterrows():
        for go_component in go_components:
            try:
                _goslim = goslim.loc[(row.uniprot_id, go_component)]
            except KeyError:
                continue
            go_column = 'go_%s' % go_component
            df.at[ind, go_column] = _goslim.go_id


def load_go_slim():
    '''
    Load GO-slim annotations from Kibeom
    '''
    # the list of GO-slim annotations
    goslim = pd.read_csv(os.path.join(DATA_DIR, 'external', 'goslim_annotations.csv'))
    goslim['go_id'] = goslim.id.apply(lambda s: s.split(':')[1])
    goslim.rename(columns={'component': 'go_type', 'label': 'go_label'}, inplace=True)
    goslim = goslim[['go_id', 'go_type', 'go_label']]

    # this is all GO annotations for all opencell baits (by uniprot_id)
    ocgo = pd.read_csv(os.path.join(DATA_DIR, 'external', 'go-annotations-for-opencell-baits.csv'))

    # GO ids are space-separated
    ocgo['go_id'] = ocgo.go_annot.str.split(' ')
    ocgo['uniprot_id'] = ocgo.uniprot

    ocgo = ocgo.explode('go_id')[['uniprot_id', 'go_id']]
    ocgo['go_id'] = ocgo.go_id.apply(lambda s: s.split(':')[1])

    # eliminate redundancies
    ocgo = ocgo.groupby(['uniprot_id', 'go_id']).first().reset_index()

    ocgoslim = pd.merge(goslim, ocgo, on='go_id', how='inner')

    return ocgoslim