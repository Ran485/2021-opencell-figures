{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting success or failure of OpenCell targets\n",
    "__Keith Cheveralls__<br>\n",
    "__October 2021__\n",
    "\n",
    "This notebook documents attempts to understand what features were important for determining a given protein could be successfully tagged using our split-FP approach to endogenous tagging. ('successful' meaning that mNeonGreen signal was detected by fluorescence microscopy).\n",
    "\n",
    "The results from this analysis are not used in the final 2021-opencell manuscript."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import scanpy as sc\n",
    "import sklearn\n",
    "\n",
    "from sklearn import inspection, metrics, model_selection\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import rcParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.settings.set_figure_params(dpi=80, facecolor='white', frameon=False)\n",
    "rcParams['font.family'] = 'sans-serif'\n",
    "rcParams['font.size'] = 12\n",
    "rcParams['legend.fontsize'] = 12\n",
    "rcParams['axes.grid'] = False\n",
    "rcParams['figure.figsize'] = (5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('../data/Suppl_Table_3_library_success.xlsx', sheet_name='library_success')\n",
    "df = df.loc[df.library_success != 'WHOLE_PROTEOME'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={s: s.lower().replace('?', '') for s in df.columns}, inplace=True)\n",
    "df.rename(columns={'is_essential': 'essential'}, inplace=True)\n",
    "\n",
    "df.essential.replace('Essential', True, inplace=True)\n",
    "df.essential.replace('Non-essential', False, inplace=True)\n",
    "\n",
    "df.library_success.replace('successful', True, inplace=True)\n",
    "df.library_success.replace('unsuccessful', False, inplace=True)\n",
    "\n",
    "# nicknames for feature columns\n",
    "df.rename(columns={'log_hek_rna_tpm': 'rna', 'log_hek_conc_nm': 'ms', 'hdr_unsorted': 'hdr'}, inplace=True)\n",
    "\n",
    "# boolean flag for n-terminal tag\n",
    "df['nterm'] = df.terminus_tagged == 'N'\n",
    "\n",
    "# drop NAs (before coerching 'essential' to boolean)\n",
    "df.dropna(axis=0, how='any', subset=['rna', 'ms', 'essential'], inplace=True)\n",
    "\n",
    "# coerce essential column to boolean\n",
    "df['essential'] = df.essential.astype(bool)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets with unsorted HDR\n",
    "df_uns = df.loc[~df.hdr.isna()].copy()\n",
    "df_uns.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean of each feature (of targets: 27% essential, 52% n-terminus, 76% successful)\n",
    "df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('library_success').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='rna', hue='library_success', stat='density', common_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='ms', hue='library_success', stat='density', common_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='ms', hue='essential', stat='density', common_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df, x='ms', hue='nterm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_uns, x='hdr', hue='library_success', stat='density', common_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uns['thresh'] = df_uns.ms > 29.3\n",
    "sns.histplot(df_uns.loc[~df_uns.library_success], x='hdr', hue='thresh', stat='density', common_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df_uns, x='hdr', hue='essential', stat='density', common_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uns.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uns.loc[~df_uns.library_success].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(\n",
    "    data=df, x='ms', y='library_success', logistic=True, n_boot=500, y_jitter=.05\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression for all targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['rna', 'ms', 'nterm', 'essential']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(df, columns, kind, split=False, balance=False):\n",
    "\n",
    "    y = df.library_success.values\n",
    "    X = df[columns].values\n",
    "\n",
    "    if split:\n",
    "        X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "            X, y, stratify=y, random_state=None\n",
    "        )\n",
    "    else:\n",
    "        X_train, X_test = X.copy(), X.copy()\n",
    "        y_train, y_test = y.copy(), y.copy()\n",
    "    \n",
    "    if kind == 'forest':\n",
    "        classifier = RandomForestClassifier(random_state=0, oob_score=True)\n",
    "    elif kind == 'logit':\n",
    "        # classifier = LogisticRegressionCV(solver='lbfgs', cv=10)\n",
    "        classifier = LogisticRegression(solver='lbfgs', class_weight=('balanced' if balance else None))\n",
    "\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict_proba(X)[:, 1]\n",
    "    y_test_pred = classifier.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    data = dict(\n",
    "        X_train=X_train, \n",
    "        X_test=X_test, \n",
    "        y_train=y_train, \n",
    "        y_test=y_test, \n",
    "        y_test_pred=y_test_pred, \n",
    "        y_pred=y_pred\n",
    "    )\n",
    "    return data, classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions for all targets from logistic regression\n",
    "res, classifier = predict(df, ['ms'], kind='logit', split=False, balance=False)\n",
    "\n",
    "res = pd.DataFrame({'y_test': res['y_test'], 'y_test_pred': res['y_test_pred']})\n",
    "sns.displot(res, x='y_test_pred', hue='y_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for all targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic with all targets\n",
    "res, classifier = predict(df, ['ms'], kind='logit', split=False, balance=False)\n",
    "\n",
    "y = res['y_test']\n",
    "y_pred = res['y_test_pred']\n",
    "print(metrics.classification_report(y, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.predict([[27.39]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average HDR efficiency for failed targets predicted to work, successful targets predicted to fail\n",
    "(\n",
    "    df.hdr.mean(),\n",
    "    \n",
    "    # failures predicted to work and to fail\n",
    "    df.loc[(~y)].hdr.mean(), \n",
    "    df.loc[(~y) & ((y_pred > 0.5))].hdr.mean(), \n",
    "    df.loc[(~y) & ((y_pred < 0.5))].hdr.mean(), \n",
    "    \n",
    "    # successes predicted to work and fail\n",
    "    df.loc[(y)].hdr.mean(),\n",
    "    df.loc[(y) & (y_pred > 0.5)].hdr.mean(),\n",
    "    df.loc[(y) & (y_pred < 0.5)].hdr.mean(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = y\n",
    "plt.scatter(df.loc[mask].ms, df.loc[mask].hdr)\n",
    "plt.scatter(df.loc[~mask].ms, df.loc[~mask].hdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = y_pred > 0.5\n",
    "plt.scatter(df.loc[mask].ms, df.loc[mask].hdr)\n",
    "plt.scatter(df.loc[~mask].ms, df.loc[~mask].hdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fails predicted to work\n",
    "dff = df.loc[(~y) & ((y_pred > 0.5))]\n",
    "plt.scatter(dff.ms, dff.hdr, label='Failures predicted to work')\n",
    "\n",
    "# success predicted to fail\n",
    "dff = df.loc[(y) & ((y_pred < 0.5))]\n",
    "plt.scatter(dff.ms, dff.hdr, label='Successes predicted to fail')\n",
    "plt.legend()\n",
    "plt.gca().set_xlabel('protein abundance')\n",
    "plt.gca().set_ylabel('HDR efficiency')\n",
    "# plt.savefig('/Users/keith.cheveralls/Box/KC-opencell-paper/crispr-success-abundance-hdr-wrong-predictions.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results for targets w unsorted HDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, classifier = predict(df_uns, ['ms'], kind='logit', split=False)\n",
    "\n",
    "y = res['y_test']\n",
    "y_pred = res['y_test_pred']\n",
    "print(metrics.classification_report(y, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, classifier = predict(df_uns, ['ms', 'hdr'], kind='logit', split=False, balance=False)\n",
    "\n",
    "y = res['y_test']\n",
    "y_pred = res['y_test_pred']\n",
    "print(metrics.classification_report(y, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average HDR efficiency for failed targets predicted to work, successful targets predicted to fail\n",
    "(\n",
    "    df_uns.hdr.mean(),\n",
    "    df_uns.loc[(~y) & ((y_pred > 0.5))].hdr.mean(), \n",
    "    df_uns.loc[(y) & (~(y_pred > 0.5))].hdr.mean(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aside: random forest predictions for an 80-20 test set\n",
    "res, classifier = predict(df, columns, kind='forest', split=True)\n",
    "\n",
    "res = pd.DataFrame({'y_test': res['y_test'], 'y_test_pred': res['y_test_pred']})\n",
    "sns.displot(res, x='y_test_pred', hue='y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = res['y_test']\n",
    "y_pred = res['y_test_pred']\n",
    "print(metrics.classification_report(y, y_pred > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression using one column at a time and all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one column at a time\n",
    "d = df.copy()\n",
    "\n",
    "kind = 'logit'\n",
    "columns = ['rna', 'ms', 'essential', 'nterm'] #+ ['hdr']\n",
    "for column in columns:\n",
    "    \n",
    "    # using only the column\n",
    "    res, classifier = predict(d, [column], kind=kind)\n",
    "    roc = metrics.roc_auc_score(d.library_success, res['y_pred'])\n",
    "    \n",
    "    print('%s (ROC %0.2f)' % (column, roc)) \n",
    "    print(metrics.classification_report(res['y_test'], res['y_test_pred'] > 0.5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all combinations of two columns\n",
    "d = df.copy()\n",
    "\n",
    "kind = 'logit'\n",
    "columns = ['rna', 'ms', 'essential', 'nterm'] #+ ['hdr']\n",
    "for col_1 in columns:\n",
    "    for col_2 in columns:\n",
    "        if col_1 == col_2: continue\n",
    "        res, classifier = predict(d, [col_1, col_2], kind='logit')\n",
    "        roc = metrics.roc_auc_score(d.library_success, res['y_pred'])\n",
    "        \n",
    "        res, classifier = predict(d, [col_1, col_2], kind='forest')\n",
    "        oob = classifier.oob_score_\n",
    "        print(\"'%s' ROC: %d | OOB: %d\" % ([col_1, col_2], 100*roc, 100*oob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report for two columns\n",
    "res, classifier = predict(df_uns, ['hdr',], kind='logit', split=False)\n",
    "print(metrics.classification_report(res['y_test'], res['y_test_pred'] > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report for two columns\n",
    "res, classifier = predict(df, ['ms', 'nterm'], kind='logit', split=False)\n",
    "print(metrics.classification_report(res['y_test'], res['y_test_pred'] > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, classifier = predict(df_uns, ['ms',], kind='logit', split=False)\n",
    "print(metrics.classification_report(res['y_test'], res['y_test_pred'] > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, classifier = predict(df_uns, ['ms', 'nterm'], kind='logit', split=False)\n",
    "print(metrics.classification_report(res['y_test'], res['y_test_pred'] > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res, classifier = predict(df_uns, ['ms', 'hdr'], kind='logit', split=False)\n",
    "print(metrics.classification_report(res['y_test'], res['y_test_pred'] > 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report for two columns\n",
    "res, classifier = predict(df_uns, ['ms', 'hdr', 'nterm', 'rna'], kind='logit', split=False)\n",
    "print(metrics.classification_report(res['y_test'], res['y_test_pred'] > 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validated precision and recall for successes and failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_precision_recall(df, columns):\n",
    "    \n",
    "    cv_scores = {}\n",
    "    X = df[columns].values\n",
    "    y = df.library_success.values > 0\n",
    "    \n",
    "    classifier = LogisticRegression(solver='lbfgs')\n",
    "    cv = sklearn.model_selection.StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\n",
    "\n",
    "    res = sklearn.model_selection.cross_validate(\n",
    "        classifier, X, y, scoring=['accuracy', 'precision', 'recall'], cv=cv\n",
    "    )  \n",
    "    cv_scores['accuracy'] = res['test_accuracy'].mean()\n",
    "    cv_scores['success_precision'] = res['test_precision'].mean()\n",
    "    cv_scores['success_recall'] = res['test_recall'].mean()\n",
    "    \n",
    "    y = ~y\n",
    "    res = sklearn.model_selection.cross_validate(classifier, X, y, scoring=['precision', 'recall'], cv=cv)    \n",
    "    cv_scores['failure_precision'] = res['test_precision'].mean()\n",
    "    cv_scores['failure_recall'] = res['test_recall'].mean()\n",
    "    \n",
    "    for key, value in cv_scores.items():\n",
    "        cv_scores[key] = int(value*100)\n",
    "    \n",
    "    return cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_precision_recall(df, ['rna'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_precision_recall(df, ['ms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_precision_recall(df, ['ms', 'nterm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_precision_recall(df, ['ms', 'rna', 'nterm', 'essential'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unsorted HDR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_precision_recall(df_uns, ['rna'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_precision_recall(df_uns, ['ms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_precision_recall(df_uns, ['ms', 'nterm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_precision_recall(df_uns, ['ms', 'hdr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate_precision_recall(df_uns, ['ms', 'hdr', 'nterm'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances from random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = d.library_success.values\n",
    "X = d[['rna', 'ms', 'essential', 'nterm']].values\n",
    "\n",
    "classifier = RandomForestClassifier(random_state=0, oob_score=True)\n",
    "classifier.fit(X, y)\n",
    "y_pred = classifier.predict_proba(X)[:, 1]\n",
    " \n",
    "result = inspection.permutation_importance(\n",
    "    classifier, X, y, n_repeats=10, random_state=42, n_jobs=2\n",
    ")\n",
    "\n",
    "(classifier.feature_importances_, result['importances_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sklearnenv",
   "language": "python",
   "name": "sklearnenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
